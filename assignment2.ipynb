{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"goodreads_reviews_young_adult_train.json\").open() as f:\n",
    "    train = [json.loads(line) for line in f]\n",
    "\n",
    "with Path(\"goodreads_reviews_young_adult_val.json\").open() as f:\n",
    "    valid = [json.loads(line) for line in f]\n",
    "\n",
    "with Path(\"goodreads_reviews_young_adult_test.json\").open() as f:\n",
    "    test = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global bias value is: 3.763456\n"
     ]
    }
   ],
   "source": [
    "def global_bias(data: list[dict[str, str | int]]) -> float:\n",
    "    \"\"\"Return global bias value for the provided dataset.\n",
    "\n",
    "    :param data: dataset of user reviews\n",
    "    :return: global bias value\n",
    "    \"\"\"\n",
    "    return sum(line[\"rating\"] for line in data) / len(data)\n",
    "\n",
    "\n",
    "print(f\"Global bias value is: {global_bias(train):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User bias value for user '91ceb82d91493506532feb02ce751ce7' is: -0.997498\n"
     ]
    }
   ],
   "source": [
    "def user_bias(\n",
    "    data: list[dict[str, str | int]], user_id: str, global_bias_val: float | None = None\n",
    ") -> float:\n",
    "    \"\"\"Return user bias for the specified user_id.\n",
    "\n",
    "    :param data: dataset of user reviews\n",
    "    :param user_id: user id of a user within the provided dataset\n",
    "    :param global_bias_val: global bias value for the provided dataset, defaults to None\n",
    "    :return: user specific bias value\n",
    "    \"\"\"\n",
    "    if global_bias_val is None:\n",
    "        global_bias_val = global_bias(data)\n",
    "\n",
    "    user_ratings = [line[\"rating\"] for line in data if line[\"user_id\"] == user_id]\n",
    "    if not user_ratings:\n",
    "        raise KeyError(\n",
    "            f\"There is no user with the ID '{user_id}' in the dataset provided\"  # NOQA: EM102\n",
    "        )\n",
    "\n",
    "    return sum(user_ratings) / len(user_ratings) - global_bias_val\n",
    "\n",
    "\n",
    "user = \"91ceb82d91493506532feb02ce751ce7\"\n",
    "print(f\"User bias value for user '{user}' is: {user_bias(train, user):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item bias value for item '6931234' is: -0.247327\n"
     ]
    }
   ],
   "source": [
    "def item_bias(\n",
    "    data: list[dict[str, str | int]], item_id: str, global_bias_val: float | None = None\n",
    ") -> float:\n",
    "    \"\"\"Return item bias for the specified item_id.\n",
    "\n",
    "    :param data: dataset of user reviews\n",
    "    :param item_id: item id of a item within the provided dataset\n",
    "    :param global_bias_val: global bias value for the provided dataset, defaults to None\n",
    "    :return: item specific bias value\n",
    "    \"\"\"\n",
    "    if global_bias_val is None:\n",
    "        global_bias_val = global_bias(data)\n",
    "\n",
    "    item_ratings = [line[\"rating\"] for line in data if line[\"item_id\"] == item_id]\n",
    "    if not item_ratings:\n",
    "        raise KeyError(\n",
    "            f\"There is no item with the ID '{item_id}' in the dataset provided\"  # NOQA: EM102\n",
    "        )\n",
    "\n",
    "    return sum(item_ratings) / len(item_ratings) - global_bias_val\n",
    "\n",
    "\n",
    "item = \"6931234\"\n",
    "print(f\"Item bias value for item '{item}' is: {item_bias(train, item):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode all item_ids and user_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data: list[dict]) -> tuple[list[int], list[int]]:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_items, train_ratings = zip(\n",
    "    *[(line[\"user_id\"], line[\"item_id\"], line[\"rating\"]) for line in train],\n",
    "    strict=False,\n",
    ")\n",
    "train_unique_users = tuple(set(train_users))\n",
    "train_unique_items = tuple(set(train_items))\n",
    "\n",
    "# Create one hot encoding keys\n",
    "user_encoding = {user: i for i, user in enumerate(unique_users)}\n",
    "item_encoding = {item: i for i, item in enumerate(unique_items)}\n",
    "\n",
    "# Apply one hot encoding\n",
    "encoded_users = [user_encoding[user] for user in users]\n",
    "encoded_items = [item_encoding[item] for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Loss function & RMSE function for stochastic gradient descent algorithm.\n",
    "\n",
    "    :param encoded_users: List of user ids encoded to be integer indices\n",
    "    :param encoded_items: List of item ids encoded to be integer indices\n",
    "    :param ratings: List of ratings s.t. rating[i] belongs to encoded_user[i] on item[i]\n",
    "    :param q: Latent vector representation of user profiles\n",
    "    :param p: Latent vector representation of item profiles\n",
    "    :param lambda1: Weighting given to sum of squared q l2 norms\n",
    "    :param lambda2: Weighting given to sum of squared p l2 norms\n",
    "    :return: Loss value and RMSE value\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------------------------------\n",
    "    # BELOW ARE TWO VERSIONS OF THE SAME METHOD TO CALCULATE THE DIFFERENCE\n",
    "    # SUM, BUT I LEFT THE FIRST HERE COMMENTED SINCE IT IS MUCH EASIER TO\n",
    "    # READ BUT SLIGHTLY SLOWER\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # differences = []\n",
    "    # for encoded_user, encoded_item, rating in zip(\n",
    "    #     encoded_users, encoded_items, ratings, strict=False\n",
    "    # ):\n",
    "    #     r_hat_ij = q[encoded_user, :] @ p[encoded_item, :].T\n",
    "    #     print(r_hat_ij)\n",
    "    #     diff_sq = (rating - r_hat_ij) ** 2\n",
    "    #     differences.append(diff_sq)\n",
    "    # difference_sum = sum(differences)\n",
    "\n",
    "    difference_sum = sum(\n",
    "        [\n",
    "            (rating - q[encoded_user, :] @ p[encoded_item, :].T) ** 2\n",
    "            for encoded_user, encoded_item, rating in zip(\n",
    "                encoded_users, encoded_items, ratings, strict=False\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    q_norm_sum = sum(\n",
    "        np.apply_along_axis(lambda x: np.linalg.norm(x) ** 2, axis=1, arr=q)\n",
    "    )\n",
    "    p_norm_sum = sum(\n",
    "        np.apply_along_axis(lambda x: np.linalg.norm(x) ** 2, axis=1, arr=p)\n",
    "    )\n",
    "    loss_val = difference_sum + lambda1 * q_norm_sum + lambda2 * p_norm_sum\n",
    "    rmse = (difference_sum / len(ratings)) ** 0.5\n",
    "    return loss_val, rmse\n",
    "\n",
    "\n",
    "def update_factors(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    lr: float,\n",
    ") -> float:\n",
    "    for encoded_user, encoded_item, rating in zip(\n",
    "        encoded_users, encoded_items, ratings, strict=False\n",
    "    ):\n",
    "        r_hat_ij = q[encoded_user, :] @ p[encoded_item, :].T\n",
    "        diff_deriv = (rating - r_hat_ij) * -2\n",
    "        for f in range(q.shape[1]):  # For factor in k\n",
    "            q[encoded_user, f] -= lr * (\n",
    "                diff_deriv * p[encoded_item, f] + 2 * lambda1 * q[encoded_user, f]\n",
    "            )\n",
    "            p[encoded_item, f] -= lr * (\n",
    "                diff_deriv * q[encoded_user, f] + 2 * lambda2 * p[encoded_item, f]\n",
    "            )\n",
    "\n",
    "\n",
    "def SGD(  # NOQA\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    k: int = 8,\n",
    "    epochs: int = 10,\n",
    "    lr: int = 0.01,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    # Many of the variables in the loss function won't change so for\n",
    "    # Readability we will set them as constant.\n",
    "    loss_func = partial(\n",
    "        loss,\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "    )\n",
    "\n",
    "    update = partial(\n",
    "        update_factors,\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "        lr=lr,\n",
    "    )\n",
    "    unique_users = max(encoded_users) + 1\n",
    "    unique_items = max(encoded_items) + 1\n",
    "\n",
    "    np.random.seed(121017)  # NOQA: NPY002\n",
    "    q = np.random.rand(unique_users, k)  # NOQA: NPY002\n",
    "    p = np.random.rand(unique_items, k)  # NOQA: NPY002\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        update(q=q, p=p)\n",
    "        loss_val, rmse_val = loss_func(q=q, p=p)\n",
    "        print(f\"Loss: {loss_val:.6f}, RMSE: {rmse_val:.6f}\")\n",
    "    return q, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2327037.907780, RMSE: 1.210368\n",
      "Loss: 2063437.201417, RMSE: 1.128759\n",
      "Loss: 1947233.502302, RMSE: 1.090019\n",
      "Loss: 1880497.811120, RMSE: 1.066684\n",
      "Loss: 1837136.633370, RMSE: 1.050959\n",
      "Loss: 1806820.497128, RMSE: 1.039626\n",
      "Loss: 1784548.632534, RMSE: 1.031077\n",
      "Loss: 1767577.485113, RMSE: 1.024408\n",
      "Loss: 1754267.121117, RMSE: 1.019066\n",
      "Loss: 1743574.250589, RMSE: 1.014693\n"
     ]
    }
   ],
   "source": [
    "q, p = SGD(\n",
    "    encoded_users=encoded_users,\n",
    "    encoded_items=encoded_items,\n",
    "    ratings=ratings,\n",
    "    lambda1=0.3,\n",
    "    lambda2=0.3,\n",
    "    k=8,\n",
    "    epochs=10,\n",
    "    lr=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2959345.483452, RMSE: 1.401605\n",
      "Loss: 2397135.970727, RMSE: 1.248147\n",
      "Loss: 2159832.992520, RMSE: 1.175582\n",
      "Loss: 2027676.180905, RMSE: 1.132135\n",
      "Loss: 1943963.289290, RMSE: 1.103059\n",
      "Loss: 1886811.866674, RMSE: 1.082280\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 28\u001b[0m\n\u001b[1;32m     18\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m     19\u001b[0m     loss,\n\u001b[1;32m     20\u001b[0m     encoded_users\u001b[38;5;241m=\u001b[39mval_encoded_users,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     lambda2\u001b[38;5;241m=\u001b[39mlambda2,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     q, p \u001b[38;5;241m=\u001b[39m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoded_users\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_users\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoded_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mratings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mratings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlambda1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlambda2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     _, rmse_val \u001b[38;5;241m=\u001b[39m validation_loss(q\u001b[38;5;241m=\u001b[39mq, p\u001b[38;5;241m=\u001b[39mp)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 118\u001b[0m, in \u001b[0;36mSGD\u001b[0;34m(encoded_users, encoded_items, ratings, lambda1, lambda2, k, epochs, lr)\u001b[0m\n\u001b[1;32m    115\u001b[0m p \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(unique_items, k)  \u001b[38;5;66;03m# NOQA: NPY002\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 118\u001b[0m     \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     loss_val, rmse_val \u001b[38;5;241m=\u001b[39m loss_func(q\u001b[38;5;241m=\u001b[39mq, p\u001b[38;5;241m=\u001b[39mp)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 75\u001b[0m, in \u001b[0;36mupdate_factors\u001b[0;34m(encoded_users, encoded_items, ratings, q, p, lambda1, lambda2, lr)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):  \u001b[38;5;66;03m# For factor in k\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     q[encoded_user, f] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     73\u001b[0m         diff_deriv \u001b[38;5;241m*\u001b[39m p[encoded_item, f] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m lambda1 \u001b[38;5;241m*\u001b[39m q[encoded_user, f]\n\u001b[1;32m     74\u001b[0m     )\n\u001b[0;32m---> 75\u001b[0m     p[encoded_item, f] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     76\u001b[0m         diff_deriv \u001b[38;5;241m*\u001b[39m q[encoded_user, f] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m lambda2 \u001b[38;5;241m*\u001b[39m p[encoded_item, f]\n\u001b[1;32m     77\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda1 = lambda2 = 0.3\n",
    "\n",
    "val_users, val_items, val_ratings = zip(\n",
    "    *[(line[\"user_id\"], line[\"item_id\"], line[\"rating\"]) for line in valid],\n",
    "    strict=False,\n",
    ")\n",
    "val_unique_users = tuple(set(val_users))\n",
    "val_unique_items = tuple(set(val_items))\n",
    "\n",
    "# Create one hot encoding keys\n",
    "val_user_encoding = {user: i for i, user in enumerate(val_unique_users)}\n",
    "val_item_encoding = {item: i for i, item in enumerate(val_unique_items)}\n",
    "\n",
    "# Apply one hot encoding\n",
    "val_encoded_users = [val_user_encoding[user] for user in val_users]\n",
    "val_encoded_items = [val_item_encoding[item] for item in val_items]\n",
    "\n",
    "validation_loss = partial(\n",
    "    loss,\n",
    "    encoded_users=val_encoded_users,\n",
    "    encoded_items=val_encoded_items,\n",
    "    ratings=val_ratings,\n",
    "    lambda1=lambda1,\n",
    "    lambda2=lambda2,\n",
    ")\n",
    "\n",
    "for k in (4, 8, 16):\n",
    "    q, p = SGD(\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "        k=k,\n",
    "        epochs=10,\n",
    "        lr=0.01,\n",
    "    )\n",
    "\n",
    "    _, rmse_val = validation_loss(q=q, p=p)\n",
    "\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"RMSE with k = {k} factors: {rmse_val:.6f}\")\n",
    "    print(\"=\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
