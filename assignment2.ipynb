{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"goodreads_reviews_young_adult_train.json\").open() as f:\n",
    "    train = [json.loads(line) for line in f]\n",
    "\n",
    "with Path(\"goodreads_reviews_young_adult_val.json\").open() as f:\n",
    "    valid = [json.loads(line) for line in f]\n",
    "\n",
    "with Path(\"goodreads_reviews_young_adult_test.json\").open() as f:\n",
    "    test = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode all data\n",
    "\n",
    "The data comes in a format that is not easily manipulatable in matrix format. As a result we encode all users and items to unique index values starting at 0 and increasing, using a process called one hot encoding.\n",
    "It is a reasonable assumption that the encodings for all known users will be known at the time of training, and that any unseen customers will be encoded to have an id that is not currently taken by another user, so creating one set of encodings should not impact our results using any unknown information. Since the weights of items and users will not be updated unless a rating is seen and loss gradient calculations will not be affected, the weights calculated will be safe from unknown data despite pre encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(\n",
    "    data: list[dict], encodings: tuple[dict, dict] | None = None\n",
    ") -> tuple[list[int], list[int], list[int], tuple[dict, dict]]:\n",
    "    \"\"\"One hot encode a list of dictionaries containing User ids & Item ids and their corresponding ratings.\n",
    "\n",
    "    :param data: List of dictionaries containing a 'user_id', 'item_id' and 'rating' attribute\n",
    "    :param encodings: A tuple of one hot encoded user & item id's to update, defaults to None\n",
    "    :return: A tuple of updated one hot encoded user & item id's\n",
    "    \"\"\"\n",
    "    users, items, ratings = zip(\n",
    "        *[(line[\"user_id\"], line[\"item_id\"], line[\"rating\"]) for line in data],\n",
    "        strict=False,\n",
    "    )\n",
    "\n",
    "    unique_users = set(users)\n",
    "    unique_items = set(items)\n",
    "\n",
    "    if encodings is None:\n",
    "        print(\"Creating encoding dicts\")\n",
    "        # Create one hot encoding keys\n",
    "        user_encoding = {user: i for i, user in enumerate(unique_users)}\n",
    "        item_encoding = {item: i for i, item in enumerate(unique_items)}\n",
    "    else:\n",
    "        user_encoding, item_encoding = encodings\n",
    "        new_users = unique_users - set(user_encoding.keys())\n",
    "        new_items = unique_items - set(item_encoding.keys())\n",
    "        max_user = max(user_encoding.values())\n",
    "        max_item = max(item_encoding.values())\n",
    "        for i, user in enumerate(new_users):\n",
    "            user_encoding[user] = max_user + i + 1\n",
    "        for i, item in enumerate(new_items):\n",
    "            item_encoding[item] = max_item + i + 1\n",
    "\n",
    "    # Apply one hot encoding\n",
    "    encoded_users = [user_encoding[user] for user in users]\n",
    "    encoded_items = [item_encoding[item] for item in items]\n",
    "    return encoded_users, encoded_items, ratings, (user_encoding, item_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating encoding dicts\n"
     ]
    }
   ],
   "source": [
    "train_users, train_items, train_ratings, encodings = encode_data(train)\n",
    "valid_users, valid_items, valid_ratings, encodings = encode_data(valid, encodings)\n",
    "test_users, test_items, test_ratings, encodings = encode_data(test, encodings)\n",
    "user_encoding, item_encoding = encodings\n",
    "reverse_user_encoding = {val: key for key, val in user_encoding.items()}\n",
    "reverse_item_encoding = {val: key for key, val in item_encoding.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global bias value is: 3.763456\n"
     ]
    }
   ],
   "source": [
    "def global_bias(ratings: list[int]) -> float:\n",
    "    \"\"\"Return global bias value for the provided dataset.\n",
    "\n",
    "    :param ratings: List of user ratings\n",
    "    :return: global bias value\n",
    "    \"\"\"\n",
    "    return sum(ratings) / len(ratings)\n",
    "\n",
    "\n",
    "global_bias_val = global_bias(train_ratings)\n",
    "print(f\"Global bias value is: {global_bias_val:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the global bias for the dataset by finding the average rating across all users and items. We find the average rating across all items is roughly 3.76.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User bias value for user '91ceb82d91493506532feb02ce751ce7' is: -0.997498\n"
     ]
    }
   ],
   "source": [
    "def user_biases(\n",
    "    ratings: list[int], encoded_users: list[int], global_bias_val: float\n",
    ") -> dict[int, float]:\n",
    "    \"\"\"Calculate the bias values observed for a list of users and their corresponding ratings.\n",
    "\n",
    "    :param ratings: A list of rating values in the same order as the input encoded_users\n",
    "    :param encoded_users: A list of one hot encoded user id's\n",
    "    :param global_bias_val: The precalculated global bias\n",
    "    :return: A dictionary of one hot encoded user ids and their corresponding biases\n",
    "    \"\"\"\n",
    "\n",
    "    user_ratings = {user: [] for user in set(encoded_users)}\n",
    "    for user, rating in zip(encoded_users, ratings, strict=True):\n",
    "        user_ratings[user].append(rating)\n",
    "\n",
    "    return {\n",
    "        user: (sum(rating_vals) / len(rating_vals)) - global_bias_val\n",
    "        if rating_vals\n",
    "        else 0\n",
    "        for user, rating_vals in user_ratings.items()\n",
    "    }\n",
    "\n",
    "\n",
    "train_user_bias = user_biases(train_ratings, train_users, global_bias_val)\n",
    "example_user = \"91ceb82d91493506532feb02ce751ce7\"\n",
    "print(\n",
    "    f\"User bias value for user '{example_user}' is: {train_user_bias[user_encoding[example_user]]:.6f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the global bias we calculate the average rating for the bias, however for the user bias we calculate the average for each individual user across all items they rated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item bias value for item '6931234' is: -0.247327\n"
     ]
    }
   ],
   "source": [
    "def item_biases(\n",
    "    ratings: list[int], encoded_items: list[int], global_bias_val: float\n",
    ") -> dict[int, float]:\n",
    "    \"\"\"Calculate the bias values observed for a list of items and their corresponding ratings.\n",
    "\n",
    "    :param ratings: A list of rating values in the same order as the input encoded_items\n",
    "    :param encoded_items: A list of one hot encoded item id's\n",
    "    :param global_bias_val: The precalculated global bias\n",
    "    :return: A dictionary of one hot encoded item ids and their corresponding biases\n",
    "    \"\"\"\n",
    "\n",
    "    item_ratings = {item: [] for item in set(encoded_items)}\n",
    "    for item, rating in zip(encoded_items, ratings, strict=True):\n",
    "        item_ratings[item].append(rating)\n",
    "\n",
    "    return {\n",
    "        item: (sum(rating_vals) / len(rating_vals)) - global_bias_val\n",
    "        if rating_vals\n",
    "        else 0\n",
    "        for item, rating_vals in item_ratings.items()\n",
    "    }\n",
    "\n",
    "\n",
    "train_item_bias = item_biases(train_ratings, train_items, global_bias_val)\n",
    "example_item = \"6931234\"\n",
    "print(\n",
    "    f\"item bias value for item '{example_item}' is: {train_item_bias[item_encoding[example_item]]:.6f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for item bias we calculate the average rating for each item regardless of the users who rated them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode all item_ids and user_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Loss function & RMSE function for stochastic gradient descent algorithm.\n",
    "\n",
    "    :param encoded_users: List of user ids encoded to be integer indices\n",
    "    :param encoded_items: List of item ids encoded to be integer indices\n",
    "    :param ratings: List of ratings s.t. rating[i] belongs to encoded_user[i] on item[i]\n",
    "    :param q: Latent vector representation of user profiles\n",
    "    :param p: Latent vector representation of item profiles\n",
    "    :param lambda1: Weighting given to sum of squared q l2 norms\n",
    "    :param lambda2: Weighting given to sum of squared p l2 norms\n",
    "    :return: Loss value and RMSE value\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------------------------------\n",
    "    # BELOW ARE TWO VERSIONS OF THE SAME METHOD TO CALCULATE THE DIFFERENCE\n",
    "    # SUM, BUT I LEFT THE FIRST HERE COMMENTED SINCE IT IS MUCH EASIER TO\n",
    "    # READ BUT SLIGHTLY SLOWER\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # differences = []\n",
    "    # for encoded_user, encoded_item, rating in zip(\n",
    "    #     encoded_users, encoded_items, ratings, strict=False\n",
    "    # ):\n",
    "    #     r_hat_ij = q[encoded_user, :] @ p[encoded_item, :].T\n",
    "    #     diff_sq = (rating - r_hat_ij) ** 2\n",
    "    #     differences.append(diff_sq)\n",
    "    # difference_sum = sum(differences)\n",
    "\n",
    "    difference_sum = sum(\n",
    "        [\n",
    "            (rating - q[encoded_user, :] @ p[encoded_item, :].T) ** 2\n",
    "            for encoded_user, encoded_item, rating in zip(\n",
    "                encoded_users, encoded_items, ratings, strict=False\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    q_norm_sum = sum(\n",
    "        np.apply_along_axis(lambda x: np.linalg.norm(x) ** 2, axis=1, arr=q)\n",
    "    )\n",
    "    p_norm_sum = sum(\n",
    "        np.apply_along_axis(lambda x: np.linalg.norm(x) ** 2, axis=1, arr=p)\n",
    "    )\n",
    "    loss_val = difference_sum + lambda1 * q_norm_sum + lambda2 * p_norm_sum\n",
    "    rmse = (difference_sum / len(ratings)) ** 0.5\n",
    "    return loss_val, rmse\n",
    "\n",
    "\n",
    "def update_factors(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    lr: float,\n",
    ") -> None:\n",
    "    \"\"\"Update the given q & p weights matrices using stochastic gradient descent.\n",
    "\n",
    "    :param encoded_users: A list of one hot encoded user ids\n",
    "    :param encoded_items: A list of one hot encoded item ids\n",
    "    :param ratings: A list of ratings in corresponding order to the encoded user and item lists\n",
    "    :param q: Weights matrix with (# users x k factors) size\n",
    "    :param p: Weights matrix with (# items x k factors) size\n",
    "    :param lambda1: Weighting given to the squared l2 norm of the q matrix\n",
    "    :param lambda2: Weighting given to the squared l2 norm of the p matrix\n",
    "    :param lr: Learning rate for weight updates\n",
    "    \"\"\"\n",
    "    for encoded_user, encoded_item, rating in zip(\n",
    "        encoded_users, encoded_items, ratings, strict=False\n",
    "    ):\n",
    "        r_hat_ij = np.dot(q[encoded_user, :].T, p[encoded_item, :])\n",
    "        diff_deriv = (rating - r_hat_ij) * -2\n",
    "        if np.isnan(diff_deriv):\n",
    "            print(r_hat_ij)\n",
    "        for f in range(q.shape[1]):  # For factor in k\n",
    "            q[encoded_user, f] -= ((lr * diff_deriv) * p[encoded_item, f]) + (\n",
    "                lr * 2 * lambda1 * q[encoded_user, f]\n",
    "            )\n",
    "            p[encoded_item, f] -= ((lr * diff_deriv) * q[encoded_user, f]) + (\n",
    "                lr * 2 * lambda2 * p[encoded_item, f]\n",
    "            )\n",
    "\n",
    "\n",
    "def SGD(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 0.01,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Stochastic Gradient Descent Latent Factor Recommender System.\n",
    "\n",
    "    :param encoded_users: A list of one hot encoded user ids\n",
    "    :param encoded_items: A list of one hot encoded item ids\n",
    "    :param ratings: A list of ratings in corresponding order to the encoded user and item lists\n",
    "    :param lambda1: Weighting given to the squared l2 norm of the q matrix\n",
    "    :param lambda2: Weighting given to the squared l2 norm of the p matrix\n",
    "    :param q: Weights matrix with (# users x k factors) size\n",
    "    :param p: Weights matrix with (# items x k factors) size\n",
    "    :param epochs: Number of iterations, defaults to 10\n",
    "    :param lr: Learning rate for weight updates, defaults to 0.01\n",
    "    :return: Final q & p matrices\n",
    "    \"\"\"\n",
    "    # Many of the variables in the loss function won't change so for\n",
    "    # Readability we will set them as constant.\n",
    "    loss_func = partial(\n",
    "        loss,\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "    )\n",
    "\n",
    "    update = partial(\n",
    "        update_factors,\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}:\", end=\" \")\n",
    "        update(q=q, p=p)\n",
    "        loss_val, rmse_val = loss_func(q=q, p=p)\n",
    "        print(f\"Loss: {loss_val:.6f}, RMSE: {rmse_val:.6f}\")\n",
    "    return q, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the next two steps easier, I implemented SGD as a standalone function as defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 2984864.076879, RMSE: 1.284621\n",
      "Epoch 2: Loss: 2632810.959652, RMSE: 1.195504\n",
      "Epoch 3: Loss: 2459863.449682, RMSE: 1.151260\n",
      "Epoch 4: Loss: 2347784.007002, RMSE: 1.122609\n",
      "Epoch 5: Loss: 2266290.048069, RMSE: 1.101856\n",
      "Epoch 6: Loss: 2203292.137349, RMSE: 1.085896\n",
      "Epoch 7: Loss: 2152741.475028, RMSE: 1.073169\n",
      "Epoch 8: Loss: 2111159.677850, RMSE: 1.062774\n",
      "Epoch 9: Loss: 2076348.943256, RMSE: 1.054138\n",
      "Epoch 10: Loss: 2046823.587465, RMSE: 1.046875\n"
     ]
    }
   ],
   "source": [
    "k = 8\n",
    "\n",
    "unique_users = max(user_encoding.values()) + 1\n",
    "unique_items = max(item_encoding.values()) + 1\n",
    "\n",
    "np.random.seed(121017)  # NOQA: NPY002\n",
    "q = np.random.normal(1, scale=0.1, size=unique_users * k).reshape(unique_users, k)\n",
    "p = np.random.normal(1, scale=0.1, size=unique_items * k).reshape(unique_items, k)\n",
    "q, p = SGD(\n",
    "    encoded_users=train_users,\n",
    "    encoded_items=train_items,\n",
    "    ratings=train_ratings,\n",
    "    lambda1=0.3,\n",
    "    lambda2=0.3,\n",
    "    q=q,\n",
    "    p=p,\n",
    "    epochs=10,\n",
    "    lr=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 2093227.624406, RMSE: 1.101478\n",
      "Epoch 2: Loss: 1998061.856147, RMSE: 1.072471\n",
      "Epoch 3: Loss: 1945491.030523, RMSE: 1.056290\n",
      "Epoch 4: Loss: 1910613.962578, RMSE: 1.045501\n",
      "Epoch 5: Loss: 1885460.403815, RMSE: 1.037697\n",
      "Epoch 6: Loss: 1866426.755245, RMSE: 1.031781\n",
      "Epoch 7: Loss: 1851565.837149, RMSE: 1.027158\n",
      "Epoch 8: Loss: 1839703.175067, RMSE: 1.023465\n",
      "Epoch 9: Loss: 1830075.425107, RMSE: 1.020466\n",
      "Epoch 10: Loss: 1822158.879828, RMSE: 1.018000\n",
      "========================================\n",
      "RMSE with k = 4 factors: 1.152184\n",
      "========================================\n",
      "Epoch 1: Loss: 2984864.076879, RMSE: 1.284621\n",
      "Epoch 2: Loss: 2632810.959652, RMSE: 1.195504\n",
      "Epoch 3: Loss: 2459863.449682, RMSE: 1.151260\n",
      "Epoch 4: Loss: 2347784.007002, RMSE: 1.122609\n",
      "Epoch 5: Loss: 2266290.048069, RMSE: 1.101856\n",
      "Epoch 6: Loss: 2203292.137349, RMSE: 1.085896\n",
      "Epoch 7: Loss: 2152741.475028, RMSE: 1.073169\n",
      "Epoch 8: Loss: 2111159.677850, RMSE: 1.062774\n",
      "Epoch 9: Loss: 2076348.943256, RMSE: 1.054138\n",
      "Epoch 10: Loss: 2046823.587465, RMSE: 1.046875\n",
      "========================================\n",
      "RMSE with k = 8 factors: 1.237186\n",
      "========================================\n",
      "Epoch 1: Loss: 5461361.260614, RMSE: 1.739878\n",
      "Epoch 2: Loss: 4473897.599651, RMSE: 1.546474\n",
      "Epoch 3: Loss: 3975264.356913, RMSE: 1.442611\n",
      "Epoch 4: Loss: 3601928.264500, RMSE: 1.360287\n",
      "Epoch 5: Loss: 3298142.347239, RMSE: 1.289527\n",
      "Epoch 6: Loss: 3061421.617351, RMSE: 1.232143\n",
      "Epoch 7: Loss: 2881929.826446, RMSE: 1.187591\n",
      "Epoch 8: Loss: 2744978.395580, RMSE: 1.153228\n",
      "Epoch 9: Loss: 2638332.644020, RMSE: 1.126431\n",
      "Epoch 10: Loss: 2553346.722367, RMSE: 1.105188\n",
      "========================================\n",
      "RMSE with k = 16 factors: 1.520210\n",
      "========================================\n",
      "\n",
      "************************************************************************************************\n",
      "Best model had k = 4 factors, Validation RMSE: 1.1521835521269135, Test RMSE: 1.1509547339374606\n",
      "************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "lambda1 = lambda2 = 0.3\n",
    "\n",
    "# Set all unchanging variables as static for future simplicity\n",
    "validation_loss = partial(\n",
    "    loss,\n",
    "    encoded_users=valid_users,\n",
    "    encoded_items=valid_items,\n",
    "    ratings=valid_ratings,\n",
    "    lambda1=lambda1,\n",
    "    lambda2=lambda2,\n",
    ")\n",
    "\n",
    "# Set all unchanging variables as static for future simplicity\n",
    "test_loss = partial(\n",
    "    loss,\n",
    "    encoded_users=test_users,\n",
    "    encoded_items=test_items,\n",
    "    ratings=test_ratings,\n",
    "    lambda1=lambda1,\n",
    "    lambda2=lambda2,\n",
    ")\n",
    "\n",
    "models = []\n",
    "\n",
    "for k in (4, 8, 16):\n",
    "    np.random.seed(121017)  # NOQA: NPY002\n",
    "    q = np.random.normal(1, 0.1, size=unique_users * k).reshape(unique_users, k)  # NOQA: NPY002\n",
    "    p = np.random.normal(1, 0.1, size=unique_items * k).reshape(unique_items, k)  # NOQA: NPY002\n",
    "    q, p = SGD(\n",
    "        encoded_users=train_users,\n",
    "        encoded_items=train_items,\n",
    "        ratings=train_ratings,\n",
    "        lambda1=0.3,\n",
    "        lambda2=0.3,\n",
    "        q=q,\n",
    "        p=p,\n",
    "        epochs=10,\n",
    "        lr=0.01,\n",
    "    )\n",
    "\n",
    "    # Store all the model weights and the validation RMSE so we can assess the best model\n",
    "    _, rmse_val = validation_loss(q=q, p=p)\n",
    "    models.append({\"k\": k, \"q\": np.copy(q), \"p\": np.copy(p), \"RMSE\": rmse_val})\n",
    "\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"RMSE with k = {k} factors: {rmse_val:.6f}\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "# Sort the models based on the validation RMSE and take the model with the lowest error\n",
    "best_model = sorted(models, key=lambda x: x[\"RMSE\"])[0]\n",
    "\n",
    "_, rmse_test = test_loss(q=best_model[\"q\"], p=best_model[\"p\"])\n",
    "\n",
    "result_str = f\"Best model had k = {best_model['k']} factors, Validation RMSE: {best_model['RMSE']}, Test RMSE: {rmse_test}\"\n",
    "print()\n",
    "print(\"*\" * len(result_str))\n",
    "print(result_str)\n",
    "print(\"*\" * len(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_loss(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    b_user: np.ndarray,\n",
    "    b_item: np.ndarray,\n",
    "    b_global: float,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    lambda3: float,\n",
    "    lambda4: float,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Loss function & RMSE function for stochastic gradient descent algorithm with bias.\n",
    "\n",
    "    :param encoded_users: List of user ids encoded to be integer indices\n",
    "    :param encoded_items: List of item ids encoded to be integer indices\n",
    "    :param ratings: List of ratings s.t. rating[i] belongs to encoded_user[i] on item[i]\n",
    "    :param q: Latent vector representation of user profiles\n",
    "    :param p: Latent vector representation of item profiles\n",
    "    :param b_user: Array of user biases s.t. each index i corresponds to one hot encoded user i\n",
    "    :param b_item: Array of item biases s.t. each index i corresponds to one hot encoded item i\n",
    "    :param b_global: Global bias for the dataset\n",
    "    :param lambda1: Weighting given to sum of squared q l2 norms\n",
    "    :param lambda2: Weighting given to sum of squared p l2 norms\n",
    "    :param lambda3: Weighting given to sum of user bias values\n",
    "    :param lambda4: Weighting given to sum of item bias values\n",
    "    :return: tuple containing total loss & RMSE\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # BELOW ARE TWO VERSIONS OF THE SAME METHOD TO CALCULATE THE DIFFERENCE\n",
    "    # SUM, BUT I LEFT THE FIRST HERE COMMENTED SINCE IT IS MUCH EASIER TO\n",
    "    # READ BUT SLIGHTLY SLOWER\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # differences = []\n",
    "    # for encoded_user, encoded_item, rating in zip(\n",
    "    #     encoded_users, encoded_items, ratings, strict=False\n",
    "    # ):\n",
    "    #     r_hat_ij = q[encoded_user, :] @ p[encoded_item, :].T\n",
    "    #     diff_sq = (rating - r_hat_ij - b_global - b_user[encoded_user] - b_item[encoded_item]) ** 2\n",
    "    #     differences.append(diff_sq)\n",
    "    # difference_sum = sum(differences)\n",
    "\n",
    "    difference_sum = sum(\n",
    "        [\n",
    "            (\n",
    "                rating\n",
    "                - q[encoded_user, :] @ p[encoded_item, :].T\n",
    "                - b_global\n",
    "                - b_user[encoded_user]\n",
    "                - b_item[encoded_item]\n",
    "            )\n",
    "            ** 2\n",
    "            for encoded_user, encoded_item, rating in zip(\n",
    "                encoded_users, encoded_items, ratings, strict=False\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    q_norm_sum = sum(\n",
    "        np.apply_along_axis(lambda x: np.linalg.norm(x) ** 2, axis=1, arr=q)\n",
    "    )\n",
    "    p_norm_sum = sum(\n",
    "        np.apply_along_axis(lambda x: np.linalg.norm(x) ** 2, axis=1, arr=p)\n",
    "    )\n",
    "\n",
    "    # user_bias_sum = sum(\n",
    "    #     [bias**2 for i, bias in enumerate(b_user) if i in encoded_users]\n",
    "    # )\n",
    "    # item_bias_sum = sum(\n",
    "    #     [bias**2 for i, bias in enumerate(b_item) if i in encoded_items]\n",
    "    # )\n",
    "\n",
    "    # I decided to ignore the user and item bias sums to speed up calculations\n",
    "    # Since the loss value is just an interesting extra.\n",
    "    user_bias_sum = item_bias_sum = 0\n",
    "\n",
    "    loss_val = (\n",
    "        difference_sum\n",
    "        + lambda1 * q_norm_sum\n",
    "        + lambda2 * p_norm_sum\n",
    "        + lambda3 * user_bias_sum\n",
    "        + lambda4 * item_bias_sum\n",
    "    )\n",
    "    rmse = (difference_sum / len(ratings)) ** 0.5\n",
    "    return loss_val, rmse\n",
    "\n",
    "\n",
    "def bias_update_factors(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    b_user: np.ndarray,\n",
    "    b_item: np.ndarray,\n",
    "    b_global: float,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    lambda3: float,\n",
    "    lambda4: float,\n",
    "    lr: float,\n",
    ") -> None:\n",
    "    \"\"\"Update the given q, p, user bias & item bias weights matrices using stochastic gradient descent.\n",
    "\n",
    "    :param encoded_users: List of user ids encoded to be integer indices\n",
    "    :param encoded_items: List of item ids encoded to be integer indices\n",
    "    :param ratings: List of ratings s.t. rating[i] belongs to encoded_user[i] on item[i]\n",
    "    :param q: Latent vector representation of user profiles\n",
    "    :param p: Latent vector representation of item profiles\n",
    "    :param b_user: Array of user biases s.t. each index i corresponds to one hot encoded user i\n",
    "    :param b_item: Array of item biases s.t. each index i corresponds to one hot encoded item i\n",
    "    :param b_global: Global bias for the dataset\n",
    "    :param lambda1: Weighting given to sum of squared q l2 norms\n",
    "    :param lambda2: Weighting given to sum of squared p l2 norms\n",
    "    :param lambda3: Weighting given to sum of user bias values\n",
    "    :param lambda4: Weighting given to sum of item bias values\n",
    "    :param lr: Learning rate for weight updates\n",
    "    \"\"\"\n",
    "    for encoded_user, encoded_item, rating in zip(\n",
    "        encoded_users, encoded_items, ratings, strict=False\n",
    "    ):\n",
    "        r_hat_ij = (\n",
    "            (q[encoded_user, :] @ p[encoded_item, :].T)\n",
    "            + b_global\n",
    "            + b_user[encoded_user]\n",
    "            + b_item[encoded_item]\n",
    "        )\n",
    "        diff_deriv = (rating - r_hat_ij) * -2\n",
    "        for f in range(q.shape[1]):  # For factor in k\n",
    "            q[encoded_user, f] -= lr * (\n",
    "                diff_deriv * p[encoded_item, f] + 2 * lambda1 * q[encoded_user, f]\n",
    "            )\n",
    "            p[encoded_item, f] -= lr * (\n",
    "                diff_deriv * q[encoded_user, f] + 2 * lambda2 * p[encoded_item, f]\n",
    "            )\n",
    "\n",
    "        b_user[encoded_user] -= lr * (diff_deriv + 2 * lambda3 * b_user[encoded_user])\n",
    "        b_item[encoded_item] -= lr * (diff_deriv + 2 * lambda4 * b_item[encoded_item])\n",
    "\n",
    "\n",
    "def bias_SGD(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    b_user: np.ndarray,\n",
    "    b_item: np.ndarray,\n",
    "    b_global: float,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    lambda3: float,\n",
    "    lambda4: float,\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 0.01,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Stochastic Gradient Descent Latent Factor Recommender System accounting for biases.\n",
    "\n",
    "    :param encoded_users: List of user ids encoded to be integer indices\n",
    "    :param encoded_items: List of item ids encoded to be integer indices\n",
    "    :param ratings: List of ratings s.t. rating[i] belongs to encoded_user[i] on item[i]\n",
    "    :param b_user: Array of user biases s.t. each index i corresponds to one hot encoded user i\n",
    "    :param b_item: Array of item biases s.t. each index i corresponds to one hot encoded item i\n",
    "    :param b_global: Global bias for the dataset\n",
    "    :param lambda1: Weighting given to sum of squared q l2 norms\n",
    "    :param lambda2: Weighting given to sum of squared p l2 norms\n",
    "    :param lambda3: Weighting given to sum of user bias values\n",
    "    :param lambda4: Weighting given to sum of item bias values\n",
    "    :param q: Latent vector representation of user profiles\n",
    "    :param p: Latent vector representation of item profiles\n",
    "    :param epochs: Number of iterations, defaults to 10\n",
    "    :param lr: Learning rate for weight updates, defaults to 0.01\n",
    "    \"\"\"\n",
    "    # Many of the variables in the loss function won't change so for\n",
    "    # Readability we will set them as constant.\n",
    "    loss_func = partial(\n",
    "        bias_loss,\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "        lambda3=lambda3,\n",
    "        lambda4=lambda4,\n",
    "        b_global=b_global,\n",
    "    )\n",
    "\n",
    "    update = partial(\n",
    "        bias_update_factors,\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "        lambda3=lambda3,\n",
    "        lambda4=lambda4,\n",
    "        b_global=b_global,\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}:\", end=\" \")\n",
    "        update(q=q, p=p, b_user=b_user, b_item=b_item)\n",
    "        loss_val, rmse_val = loss_func(q=q, p=p, b_user=b_user, b_item=b_item)\n",
    "        print(f\"Loss: {loss_val:.6f}, RMSE: {rmse_val:.6f}\")\n",
    "    return q, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 2922948.562885, RMSE: 1.279505\n",
      "Epoch 2: Loss: 2262923.909632, RMSE: 1.096707\n",
      "Epoch 3: Loss: 2042770.140403, RMSE: 1.032366\n",
      "Epoch 4: Loss: 1936271.372365, RMSE: 1.002041\n",
      "Epoch 5: Loss: 1873952.516700, RMSE: 0.985393\n",
      "Epoch 6: Loss: 1832737.140797, RMSE: 0.975301\n",
      "Epoch 7: Loss: 1803014.207417, RMSE: 0.968731\n",
      "Epoch 8: Loss: 1780162.974455, RMSE: 0.964213\n",
      "Epoch 9: Loss: 1761724.198071, RMSE: 0.960968\n",
      "Epoch 10: Loss: 1746284.288342, RMSE: 0.958551\n",
      "\n",
      "The learned user bias value for user '91ceb82d91493506532feb02ce751ce7' is: -0.549056\n",
      "The learned item bias value for item '6931234' is: -0.157415\n"
     ]
    }
   ],
   "source": [
    "k = 8\n",
    "lambda1 = lambda2 = lambda3 = lambda4 = 0.3\n",
    "\n",
    "unique_users = max(user_encoding.values()) + 1\n",
    "unique_items = max(item_encoding.values()) + 1\n",
    "\n",
    "np.random.seed(121017)  # NOQA: NPY002\n",
    "q = np.random.normal(1, 0.1, size=unique_users * k).reshape(unique_users, k)  # NOQA: NPY002\n",
    "p = np.random.normal(1, 0.1, size=unique_items * k).reshape(unique_items, k)  # NOQA: NPY002\n",
    "b_global = global_bias_val\n",
    "b_user = np.zeros(shape=(unique_users,))\n",
    "b_item = np.zeros(shape=(unique_items,))\n",
    "for user, bias in train_user_bias.items():\n",
    "    b_user[user] = bias\n",
    "for item, bias in train_item_bias.items():\n",
    "    b_item[item] = bias\n",
    "\n",
    "q, p = bias_SGD(\n",
    "    encoded_users=train_users,\n",
    "    encoded_items=train_items,\n",
    "    ratings=train_ratings,\n",
    "    lambda1=lambda1,\n",
    "    lambda2=lambda2,\n",
    "    lambda3=lambda3,\n",
    "    lambda4=lambda4,\n",
    "    b_user=b_user,\n",
    "    b_item=b_item,\n",
    "    b_global=b_global,\n",
    "    q=q,\n",
    "    p=p,\n",
    "    epochs=10,\n",
    "    lr=0.01,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\n",
    "    f\"The learned user bias value for user '{example_user}' is: {b_user[user_encoding[example_user]]:.6f}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The learned item bias value for item '{example_item}' is: {b_item[item_encoding[example_item]]:.6f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 2119001.580241, RMSE: 1.125568\n",
      "Epoch 2: Loss: 1815408.532947, RMSE: 1.033979\n",
      "Epoch 3: Loss: 1703198.257289, RMSE: 1.000072\n",
      "Epoch 4: Loss: 1644770.695395, RMSE: 0.983126\n",
      "Epoch 5: Loss: 1608825.318478, RMSE: 0.973319\n",
      "Epoch 6: Loss: 1584294.301591, RMSE: 0.967104\n",
      "Epoch 7: Loss: 1566297.871030, RMSE: 0.962908\n",
      "Epoch 8: Loss: 1552368.432465, RMSE: 0.959936\n",
      "Epoch 9: Loss: 1541133.345162, RMSE: 0.957749\n",
      "Epoch 10: Loss: 1531772.774137, RMSE: 0.956090\n",
      "========================================\n",
      "RMSE with k = 4 factors: 1.176205\n",
      "========================================\n",
      "Epoch 1: Loss: 2922948.562885, RMSE: 1.279505\n",
      "Epoch 2: Loss: 2262923.909632, RMSE: 1.096707\n",
      "Epoch 3: Loss: 2042770.140403, RMSE: 1.032366\n",
      "Epoch 4: Loss: 1936271.372365, RMSE: 1.002041\n",
      "Epoch 5: Loss: 1873952.516700, RMSE: 0.985393\n",
      "Epoch 6: Loss: 1832737.140797, RMSE: 0.975301\n",
      "Epoch 7: Loss: 1803014.207417, RMSE: 0.968731\n",
      "Epoch 8: Loss: 1780162.974455, RMSE: 0.964213\n",
      "Epoch 9: Loss: 1761724.198071, RMSE: 0.960968\n",
      "Epoch 10: Loss: 1746284.288342, RMSE: 0.958551\n",
      "========================================\n",
      "RMSE with k = 8 factors: 1.278609\n",
      "========================================\n",
      "Epoch 1: Loss: 4014155.814383, RMSE: 1.427887\n",
      "Epoch 2: Loss: 2924569.217299, RMSE: 1.149186\n",
      "Epoch 3: Loss: 2596830.128082, RMSE: 1.057679\n",
      "Epoch 4: Loss: 2445758.631561, RMSE: 1.016917\n",
      "Epoch 5: Loss: 2358358.435449, RMSE: 0.995390\n",
      "Epoch 6: Loss: 2299851.250680, RMSE: 0.982648\n",
      "Epoch 7: Loss: 2256578.246679, RMSE: 0.974452\n",
      "Epoch 8: Loss: 2222260.053619, RMSE: 0.968831\n",
      "Epoch 9: Loss: 2193664.428488, RMSE: 0.964776\n",
      "Epoch 10: Loss: 2168977.342082, RMSE: 0.961726\n",
      "========================================\n",
      "RMSE with k = 16 factors: 1.610828\n",
      "========================================\n",
      "\n",
      "************************************************************************************************\n",
      "Best model had k = 4 factors, Validation RMSE: 1.1762052615337248, Test RMSE: 1.1755568802145808\n",
      "************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "lambda1 = lambda2 = lambda3 = lambda4 = 0.3\n",
    "\n",
    "validation_loss = partial(\n",
    "    bias_loss,\n",
    "    encoded_users=valid_users,\n",
    "    encoded_items=valid_items,\n",
    "    ratings=valid_ratings,\n",
    "    lambda1=lambda1,\n",
    "    lambda2=lambda2,\n",
    "    lambda3=lambda3,\n",
    "    lambda4=lambda4,\n",
    "    b_global=b_global,\n",
    ")\n",
    "\n",
    "test_loss = partial(\n",
    "    bias_loss,\n",
    "    encoded_users=test_users,\n",
    "    encoded_items=test_items,\n",
    "    ratings=test_ratings,\n",
    "    lambda1=lambda1,\n",
    "    lambda2=lambda2,\n",
    "    lambda3=lambda3,\n",
    "    lambda4=lambda4,\n",
    "    b_global=b_global,\n",
    ")\n",
    "\n",
    "models = []\n",
    "\n",
    "for k in (4, 8, 16):\n",
    "    np.random.seed(121017)  # NOQA: NPY002\n",
    "    q = np.random.normal(1, 0.1, size=unique_users * k).reshape(unique_users, k)  # NOQA: NPY002\n",
    "    p = np.random.normal(1, 0.1, size=unique_items * k).reshape(unique_items, k)  # NOQA: NPY002\n",
    "    b_global = global_bias_val\n",
    "    b_user = np.zeros(shape=(unique_users,))\n",
    "    b_item = np.zeros(shape=(unique_items,))\n",
    "    for user, bias in train_user_bias.items():\n",
    "        b_user[user] = bias\n",
    "    for item, bias in train_item_bias.items():\n",
    "        b_item[item] = bias\n",
    "    q, p = bias_SGD(\n",
    "        encoded_users=train_users,\n",
    "        encoded_items=train_items,\n",
    "        ratings=train_ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "        lambda3=lambda3,\n",
    "        lambda4=lambda4,\n",
    "        b_user=b_user,\n",
    "        b_item=b_item,\n",
    "        b_global=b_global,\n",
    "        q=q,\n",
    "        p=p,\n",
    "        epochs=10,\n",
    "        lr=0.01,\n",
    "    )\n",
    "\n",
    "    _, rmse_val = validation_loss(q=q, p=p, b_item=b_item, b_user=b_user)\n",
    "    models.append(\n",
    "        {\n",
    "            \"k\": k,\n",
    "            \"q\": np.copy(q),\n",
    "            \"p\": np.copy(p),\n",
    "            \"user_bias\": np.copy(b_user),\n",
    "            \"item_bias\": np.copy(b_item),\n",
    "            \"RMSE\": rmse_val,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"RMSE with k = {k} factors: {rmse_val:.6f}\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "best_model = sorted(models, key=lambda x: x[\"RMSE\"])[0]\n",
    "\n",
    "_, rmse_test = test_loss(\n",
    "    q=best_model[\"q\"],\n",
    "    p=best_model[\"p\"],\n",
    "    b_user=best_model[\"user_bias\"],\n",
    "    b_item=best_model[\"item_bias\"],\n",
    ")\n",
    "\n",
    "result_str = f\"Best model had k = {best_model['k']} factors, Validation RMSE: {best_model['RMSE']}, Test RMSE: {rmse_test}\"\n",
    "print()\n",
    "print(\"*\" * len(result_str))\n",
    "print(result_str)\n",
    "print(\"*\" * len(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the results seen in task 2b, we find the best model is the one with k = 4 factors. We found that the validation RMSE was increasing as the K parameter increased, suggesting that the system is quite simple and is best explained by fewer variables. Because of this the addition of more factors just adds to the size of the q and p matrices, thus increasing the loss. Interestingly, the test RMSE for the bias included model is around 0.025 greater than the one we created in task 2b with the same K parameter. This again indicates the simplicity of the system, being better represented by less parameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
