{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"goodreads_reviews_young_adult_train.json\").open() as f:\n",
    "    train = [json.loads(line) for line in f]\n",
    "\n",
    "with Path(\"goodreads_reviews_young_adult_val.json\").open() as f:\n",
    "    valid = [json.loads(line) for line in f]\n",
    "\n",
    "with Path(\"goodreads_reviews_young_adult_test.json\").open() as f:\n",
    "    test = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global bias value is: 3.763456\n"
     ]
    }
   ],
   "source": [
    "def global_bias(data: list[dict[str, str | int]]) -> float:\n",
    "    \"\"\"Return global bias value for the provided dataset.\n",
    "\n",
    "    :param data: dataset of user reviews\n",
    "    :return: global bias value\n",
    "    \"\"\"\n",
    "    return sum(line[\"rating\"] for line in data) / len(data)\n",
    "\n",
    "\n",
    "print(f\"Global bias value is: {global_bias(train):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User bias value for user '91ceb82d91493506532feb02ce751ce7' is: -0.997498\n"
     ]
    }
   ],
   "source": [
    "def user_bias(\n",
    "    data: list[dict[str, str | int]], user_id: str, global_bias_val: float | None = None\n",
    ") -> float:\n",
    "    \"\"\"Return user bias for the specified user_id.\n",
    "\n",
    "    :param data: dataset of user reviews\n",
    "    :param user_id: user id of a user within the provided dataset\n",
    "    :param global_bias_val: global bias value for the provided dataset, defaults to None\n",
    "    :return: user specific bias value\n",
    "    \"\"\"\n",
    "    if global_bias_val is None:\n",
    "        global_bias_val = global_bias(data)\n",
    "\n",
    "    user_ratings = [line[\"rating\"] for line in data if line[\"user_id\"] == user_id]\n",
    "    if not user_ratings:\n",
    "        raise KeyError(\n",
    "            f\"There is no user with the ID '{user_id}' in the dataset provided\"  # NOQA: EM102\n",
    "        )\n",
    "\n",
    "    return sum(user_ratings) / len(user_ratings) - global_bias_val\n",
    "\n",
    "\n",
    "user = \"91ceb82d91493506532feb02ce751ce7\"\n",
    "print(f\"User bias value for user '{user}' is: {user_bias(train, user):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item bias value for item '6931234' is: -0.247327\n"
     ]
    }
   ],
   "source": [
    "def item_bias(\n",
    "    data: list[dict[str, str | int]], item_id: str, global_bias_val: float | None = None\n",
    ") -> float:\n",
    "    \"\"\"Return item bias for the specified item_id.\n",
    "\n",
    "    :param data: dataset of user reviews\n",
    "    :param item_id: item id of a item within the provided dataset\n",
    "    :param global_bias_val: global bias value for the provided dataset, defaults to None\n",
    "    :return: item specific bias value\n",
    "    \"\"\"\n",
    "    if global_bias_val is None:\n",
    "        global_bias_val = global_bias(data)\n",
    "\n",
    "    item_ratings = [line[\"rating\"] for line in data if line[\"item_id\"] == item_id]\n",
    "    if not item_ratings:\n",
    "        raise KeyError(\n",
    "            f\"There is no item with the ID '{item_id}' in the dataset provided\"  # NOQA: EM102\n",
    "        )\n",
    "\n",
    "    return sum(item_ratings) / len(item_ratings) - global_bias_val\n",
    "\n",
    "\n",
    "item = \"6931234\"\n",
    "print(f\"Item bias value for item '{item}' is: {item_bias(train, item):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode all item_ids and user_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data: list[dict], encodings:tuple[dict,dict] | None = None) -> tuple[list[int], list[int], list[int], tuple[dict, dict]]:\n",
    "    users, items, ratings = zip(\n",
    "        *[(line[\"user_id\"], line[\"item_id\"], line[\"rating\"]) for line in data],\n",
    "        strict=False,\n",
    "    )\n",
    "\n",
    "    unique_users = set(users)\n",
    "    unique_items = set(items)\n",
    "\n",
    "    if encodings is None:\n",
    "        \n",
    "        # Create one hot encoding keys\n",
    "        user_encoding = {user: i for i, user in enumerate(unique_users)}\n",
    "        item_encoding = {item: i for i, item in enumerate(unique_items)}\n",
    "    else:\n",
    "        user_encoding, item_encoding = encodings\n",
    "        new_users = unique_users - set(user_encoding.keys())\n",
    "        new_items = unique_items - set(item_encoding.keys())\n",
    "        max_user = max(user_encoding.values())\n",
    "        max_item = max(item_encoding.values())\n",
    "        for i, user in enumerate(new_users):\n",
    "            user_encoding[user] = max_user + i + 1\n",
    "        for i, item in enumerate(new_items):\n",
    "            item_encoding[item] = max_item + i + 1\n",
    "\n",
    "    # Apply one hot encoding\n",
    "    encoded_users = [user_encoding[user] for user in users]\n",
    "    encoded_items = [item_encoding[item] for item in items]\n",
    "    return encoded_users, encoded_items, ratings, (user_encoding, item_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users, train_items, train_ratings, encodings = encode_data(train)\n",
    "valid_users, valid_items, valid_ratings, encodings = encode_data(valid, encodings)\n",
    "test_users, test_items, test_ratings, encodings = encode_data(test, encodings)\n",
    "\n",
    "user_encoding, item_encoding = encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Loss function & RMSE function for stochastic gradient descent algorithm.\n",
    "\n",
    "    :param encoded_users: List of user ids encoded to be integer indices\n",
    "    :param encoded_items: List of item ids encoded to be integer indices\n",
    "    :param ratings: List of ratings s.t. rating[i] belongs to encoded_user[i] on item[i]\n",
    "    :param q: Latent vector representation of user profiles\n",
    "    :param p: Latent vector representation of item profiles\n",
    "    :param lambda1: Weighting given to sum of squared q l2 norms\n",
    "    :param lambda2: Weighting given to sum of squared p l2 norms\n",
    "    :return: Loss value and RMSE value\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------------------------------\n",
    "    # BELOW ARE TWO VERSIONS OF THE SAME METHOD TO CALCULATE THE DIFFERENCE\n",
    "    # SUM, BUT I LEFT THE FIRST HERE COMMENTED SINCE IT IS MUCH EASIER TO\n",
    "    # READ BUT SLIGHTLY SLOWER\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # differences = []\n",
    "    # for encoded_user, encoded_item, rating in zip(\n",
    "    #     encoded_users, encoded_items, ratings, strict=False\n",
    "    # ):\n",
    "    #     r_hat_ij = q[encoded_user, :] @ p[encoded_item, :].T\n",
    "    #     diff_sq = (rating - r_hat_ij) ** 2\n",
    "    #     differences.append(diff_sq)\n",
    "    # difference_sum = sum(differences)\n",
    "\n",
    "    difference_sum = sum(\n",
    "        [\n",
    "            (rating - q[encoded_user, :] @ p[encoded_item, :].T) ** 2\n",
    "            for encoded_user, encoded_item, rating in zip(\n",
    "                encoded_users, encoded_items, ratings, strict=False\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    q_norm_sum = sum(\n",
    "        np.apply_along_axis(lambda x: np.linalg.norm(x) ** 2, axis=1, arr=q)\n",
    "    )\n",
    "    p_norm_sum = sum(\n",
    "        np.apply_along_axis(lambda x: np.linalg.norm(x) ** 2, axis=1, arr=p)\n",
    "    )\n",
    "    loss_val = difference_sum + lambda1 * q_norm_sum + lambda2 * p_norm_sum\n",
    "    rmse = (difference_sum / len(ratings)) ** 0.5\n",
    "    return loss_val, rmse\n",
    "\n",
    "\n",
    "def update_factors(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    lr: float,\n",
    ") -> float:\n",
    "    for encoded_user, encoded_item, rating in zip(\n",
    "        encoded_users, encoded_items, ratings, strict=False\n",
    "    ):\n",
    "        r_hat_ij = q[encoded_user, :] @ p[encoded_item, :].T\n",
    "        diff_deriv = (rating - r_hat_ij) * -2\n",
    "        for f in range(q.shape[1]):  # For factor in k\n",
    "            q[encoded_user, f] -= lr * (\n",
    "                diff_deriv * p[encoded_item, f] + 2 * lambda1 * q[encoded_user, f]\n",
    "            )\n",
    "            p[encoded_item, f] -= lr * (\n",
    "                diff_deriv * q[encoded_user, f] + 2 * lambda2 * p[encoded_item, f]\n",
    "            )\n",
    "\n",
    "\n",
    "def SGD(  # NOQA\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    epochs: int = 10,\n",
    "    lr: int = 0.01,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    # Many of the variables in the loss function won't change so for\n",
    "    # Readability we will set them as constant.\n",
    "    loss_func = partial(\n",
    "        loss,\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "    )\n",
    "\n",
    "    update = partial(\n",
    "        update_factors,\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        update(q=q, p=p)\n",
    "        loss_val, rmse_val = loss_func(q=q, p=p)\n",
    "        print(f\"Loss: {loss_val:.6f}, RMSE: {rmse_val:.6f}\")\n",
    "    return q, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2360828.150837, RMSE: 1.208986\n",
      "Loss: 2098238.810055, RMSE: 1.127593\n",
      "Loss: 1982786.722295, RMSE: 1.089051\n",
      "Loss: 1916613.562958, RMSE: 1.065878\n",
      "Loss: 1873682.752792, RMSE: 1.050282\n",
      "Loss: 1843707.680242, RMSE: 1.039054\n",
      "Loss: 1821714.382455, RMSE: 1.030593\n",
      "Loss: 1804976.122554, RMSE: 1.023999\n",
      "Loss: 1791864.203873, RMSE: 1.018722\n",
      "Loss: 1781343.350403, RMSE: 1.014405\n"
     ]
    }
   ],
   "source": [
    "k = 8\n",
    "\n",
    "unique_users = max(user_encoding.values()) + 1\n",
    "unique_items = max(item_encoding.values()) + 1\n",
    "\n",
    "np.random.seed(121017)  # NOQA: NPY002\n",
    "q = np.random.normal(size=unique_users*k).reshape(unique_users, k)  # NOQA: NPY002\n",
    "p = np.random.normal(size=unique_items*k).reshape(unique_items, k)  # NOQA: NPY002\n",
    "\n",
    "q, p = SGD(\n",
    "    encoded_users=train_users,\n",
    "    encoded_items=train_items,\n",
    "    ratings=train_ratings,\n",
    "    lambda1=0.3,\n",
    "    lambda2=0.3,\n",
    "    q=q,\n",
    "    p=p,\n",
    "    epochs=10,\n",
    "    lr=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2979907.660673, RMSE: 1.401940\n",
      "Loss: 2418737.452257, RMSE: 1.248822\n",
      "Loss: 2180889.557103, RMSE: 1.176139\n",
      "Loss: 2048251.818305, RMSE: 1.132562\n",
      "Loss: 1964234.719100, RMSE: 1.103397\n",
      "Loss: 1906898.031309, RMSE: 1.082562\n",
      "Loss: 1865800.471702, RMSE: 1.067014\n",
      "Loss: 1835296.618059, RMSE: 1.055043\n",
      "Loss: 1812052.892008, RMSE: 1.045601\n",
      "Loss: 1793969.321085, RMSE: 1.038010\n",
      "========================================\n",
      "RMSE with k = 4 factors: 1.271235\n",
      "========================================\n",
      "Loss: 2360828.150837, RMSE: 1.208986\n",
      "Loss: 2098238.810055, RMSE: 1.127593\n",
      "Loss: 1982786.722295, RMSE: 1.089051\n",
      "Loss: 1916613.562958, RMSE: 1.065878\n",
      "Loss: 1873682.752792, RMSE: 1.050282\n",
      "Loss: 1843707.680242, RMSE: 1.039054\n",
      "Loss: 1821714.382455, RMSE: 1.030593\n",
      "Loss: 1804976.122554, RMSE: 1.023999\n",
      "Loss: 1791864.203873, RMSE: 1.018722\n",
      "Loss: 1781343.350403, RMSE: 1.014405\n",
      "========================================\n",
      "RMSE with k = 8 factors: 1.199577\n",
      "========================================\n",
      "Loss: 2266297.274448, RMSE: 1.122188\n",
      "Loss: 2105780.043836, RMSE: 1.074445\n",
      "Loss: 2024408.493054, RMSE: 1.050121\n",
      "Loss: 1972794.160549, RMSE: 1.034777\n",
      "Loss: 1936426.733155, RMSE: 1.024069\n",
      "Loss: 1909158.690800, RMSE: 1.016133\n",
      "Loss: 1887842.757647, RMSE: 1.010008\n",
      "Loss: 1870665.497674, RMSE: 1.005139\n",
      "Loss: 1856493.470872, RMSE: 1.001176\n",
      "Loss: 1844574.899460, RMSE: 0.997888\n",
      "========================================\n",
      "RMSE with k = 16 factors: 1.164499\n",
      "========================================\n",
      "\n",
      "*************************************************************************************************\n",
      "Best model had k = 16 factors, Validation RMSE: 1.1644991118551289, Test RMSE: 1.1628731158585603\n",
      "*************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "lambda1 = lambda2 = 0.3\n",
    "\n",
    "validation_loss = partial(\n",
    "    loss,\n",
    "    encoded_users=valid_users,\n",
    "    encoded_items=valid_items,\n",
    "    ratings=valid_ratings,\n",
    "    lambda1=lambda1,\n",
    "    lambda2=lambda2,\n",
    ")\n",
    "\n",
    "test_loss = partial(\n",
    "    loss,\n",
    "    encoded_users=test_users,\n",
    "    encoded_items=test_items,\n",
    "    ratings=test_ratings,\n",
    "    lambda1=lambda1,\n",
    "    lambda2=lambda2,\n",
    ")\n",
    "\n",
    "models = []\n",
    "\n",
    "for k in (4, 8, 16):\n",
    "    np.random.seed(121017)  # NOQA: NPY002\n",
    "    q = np.random.normal(size=unique_users*k).reshape(unique_users, k)  # NOQA: NPY002\n",
    "    p = np.random.normal(size=unique_items*k).reshape(unique_items, k)  # NOQA: NPY002\n",
    "    q, p = SGD(\n",
    "        encoded_users=train_users,\n",
    "        encoded_items=train_items,\n",
    "        ratings=train_ratings,\n",
    "        lambda1=0.3,\n",
    "        lambda2=0.3,\n",
    "        q=q,\n",
    "        p=p,\n",
    "        epochs=10,\n",
    "        lr=0.01,\n",
    "    )\n",
    "\n",
    "    _, rmse_val = validation_loss(q=q, p=p)\n",
    "    models.append({\"k\": k, \"q\": np.copy(q), \"p\": np.copy(p), \"RMSE\": rmse_val})\n",
    "\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"RMSE with k = {k} factors: {rmse_val:.6f}\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "best_model = sorted(models, key=lambda x: x[\"RMSE\"])[0]\n",
    "\n",
    "_, rmse_test = test_loss(q=best_model[\"q\"],p=best_model[\"p\"])\n",
    "\n",
    "result_str = f\"Best model had k = {best_model['k']} factors, Validation RMSE: {best_model['RMSE']}, Test RMSE: {rmse_test}\"\n",
    "print()\n",
    "print(\"*\"*len(result_str))\n",
    "print(result_str)\n",
    "print(\"*\"*len(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************************************************\n",
      "Best model had k = 16 factors, Validation RMSE: 1.1644991118551289, Test RMSE: 1.1628731158585603\n",
      "*************************************************************************************************\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_loss(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    b_user: np.ndarray,\n",
    "    b_item: np.ndarray,\n",
    "    b_global: float,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    lambda3: float,\n",
    "    lambda4: float\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Loss function & RMSE function for stochastic gradient descent algorithm.\n",
    "\n",
    "    :param encoded_users: List of user ids encoded to be integer indices\n",
    "    :param encoded_items: List of item ids encoded to be integer indices\n",
    "    :param ratings: List of ratings s.t. rating[i] belongs to encoded_user[i] on item[i]\n",
    "    :param q: Latent vector representation of user profiles\n",
    "    :param p: Latent vector representation of item profiles\n",
    "    :param lambda1: Weighting given to sum of squared q l2 norms\n",
    "    :param lambda2: Weighting given to sum of squared p l2 norms\n",
    "    :return: Loss value and RMSE value\n",
    "    \"\"\"\n",
    "    # ---------------------------------------------------------------------\n",
    "    # BELOW ARE TWO VERSIONS OF THE SAME METHOD TO CALCULATE THE DIFFERENCE\n",
    "    # SUM, BUT I LEFT THE FIRST HERE COMMENTED SINCE IT IS MUCH EASIER TO\n",
    "    # READ BUT SLIGHTLY SLOWER\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # differences = []\n",
    "    # for encoded_user, encoded_item, rating in zip(\n",
    "    #     encoded_users, encoded_items, ratings, strict=False\n",
    "    # ):\n",
    "    #     r_hat_ij = q[encoded_user, :] @ p[encoded_item, :].T\n",
    "    #     diff_sq = (rating - r_hat_ij - b_global - b_user[encoded_user] - b_item[encoded_item]) ** 2\n",
    "    #     differences.append(diff_sq)\n",
    "    # difference_sum = sum(differences)\n",
    "\n",
    "    difference_sum = sum(\n",
    "        [\n",
    "            (rating - q[encoded_user, :] @ p[encoded_item, :].T - b_global - b_user[encoded_user] - b_item[encoded_item]) ** 2\n",
    "            for encoded_user, encoded_item, rating in zip(\n",
    "                encoded_users, encoded_items, ratings, strict=False\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    q_norm_sum = sum(\n",
    "        np.apply_along_axis(lambda x: np.linalg.norm(x) ** 2, axis=1, arr=q)\n",
    "    )\n",
    "    p_norm_sum = sum(\n",
    "        np.apply_along_axis(lambda x: np.linalg.norm(x) ** 2, axis=1, arr=p)\n",
    "    )\n",
    "    user_bias_sum = sum(b_user**2)\n",
    "    item_bias_sum = sum(b_item**2)\n",
    "    loss_val = difference_sum + lambda1 * q_norm_sum + lambda2 * p_norm_sum + lambda3 * user_bias_sum + lambda4 * item_bias_sum\n",
    "    rmse = (difference_sum / len(ratings)) ** 0.5\n",
    "    return loss_val, rmse\n",
    "\n",
    "\n",
    "def bias_update_factors(\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    b_user: np.ndarray,\n",
    "    b_item: np.ndarray,\n",
    "    b_global: float,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    lambda3: float,\n",
    "    lambda4: float,\n",
    "    lr: float,\n",
    ") -> float:\n",
    "    for encoded_user, encoded_item, rating in zip(\n",
    "        encoded_users, encoded_items, ratings, strict=False\n",
    "    ):\n",
    "        r_hat_ij = q[encoded_user, :] @ p[encoded_item, :].T - b_global - b_user[encoded_user] - b_item[encoded_item]\n",
    "        diff_deriv = (rating - r_hat_ij) * -2\n",
    "        for f in range(q.shape[1]):  # For factor in k\n",
    "            q[encoded_user, f] -= lr * (\n",
    "                diff_deriv * p[encoded_item, f] + 2 * lambda1 * q[encoded_user, f]\n",
    "            )\n",
    "            p[encoded_item, f] -= lr * (\n",
    "                diff_deriv * q[encoded_user, f] + 2 * lambda2 * p[encoded_item, f]\n",
    "            )\n",
    "        \n",
    "        b_user[encoded_user] -= lr*(diff_deriv + 2*lambda3 * b_user[encoded_user])\n",
    "        b_item[encoded_item] -= lr*(diff_deriv + 2*lambda4 * b_item[encoded_item])\n",
    "\n",
    "\n",
    "def bias_SGD(  # NOQA\n",
    "    encoded_users: list[int],\n",
    "    encoded_items: list[int],\n",
    "    ratings: list[int],\n",
    "    b_user: np.ndarray,\n",
    "    b_item: np.ndarray,\n",
    "    b_global: float,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    "    lambda3: float,\n",
    "    lambda4: float,\n",
    "    q: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    epochs: int = 10,\n",
    "    lr: int = 0.01,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    # Many of the variables in the loss function won't change so for\n",
    "    # Readability we will set them as constant.\n",
    "    loss_func = partial(\n",
    "        bias_loss,\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "        lambda3=lambda3,\n",
    "        lambda4=lambda4,\n",
    "        b_global=b_global\n",
    "    )\n",
    "\n",
    "    update = partial(\n",
    "        bias_update_factors,\n",
    "        encoded_users=encoded_users,\n",
    "        encoded_items=encoded_items,\n",
    "        ratings=ratings,\n",
    "        lambda1=lambda1,\n",
    "        lambda2=lambda2,\n",
    "        lambda3=lambda3,\n",
    "        lambda4=lambda4,\n",
    "        b_global=b_global,\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        update(q=q, p=p, b_user=b_user, b_item=b_item)\n",
    "        loss_val, rmse_val = loss_func(q=q, p=p, b_user=b_user, b_item=b_item, b_global=b_global)\n",
    "        print(f\"Loss: {loss_val:.6f}, RMSE: {rmse_val:.6f}\")\n",
    "    return q, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1876/4195626885.py:87: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  diff_deriv * q[encoded_user, f] + 2 * lambda2 * p[encoded_item, f]\n",
      "/tmp/ipykernel_1876/4195626885.py:80: RuntimeWarning: invalid value encountered in matmul\n",
      "  r_hat_ij = q[encoded_user, :] @ p[encoded_item, :].T - b_global - b_user[encoded_user] - b_item[encoded_item]\n",
      "/tmp/ipykernel_1876/4195626885.py:84: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  diff_deriv * p[encoded_item, f] + 2 * lambda1 * q[encoded_user, f]\n",
      "/tmp/ipykernel_1876/4195626885.py:87: RuntimeWarning: invalid value encountered in scalar add\n",
      "  diff_deriv * q[encoded_user, f] + 2 * lambda2 * p[encoded_item, f]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m b_item \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(size\u001b[38;5;241m=\u001b[39munique_items))  \u001b[38;5;66;03m# NOQA: NPY002\u001b[39;00m\n\u001b[1;32m     12\u001b[0m b_global \u001b[38;5;241m=\u001b[39m global_bias(train)\n\u001b[0;32m---> 14\u001b[0m q, p \u001b[38;5;241m=\u001b[39m \u001b[43mbias_SGD\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_users\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_users\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mratings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ratings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda3\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda4\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb_user\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb_item\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_item\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 138\u001b[0m, in \u001b[0;36mbias_SGD\u001b[0;34m(encoded_users, encoded_items, ratings, b_user, b_item, b_global, lambda1, lambda2, lambda3, lambda4, q, p, epochs, lr)\u001b[0m\n\u001b[1;32m    124\u001b[0m update \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    125\u001b[0m     bias_update_factors,\n\u001b[1;32m    126\u001b[0m     encoded_users\u001b[38;5;241m=\u001b[39mencoded_users,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 138\u001b[0m     \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_user\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_item\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_item\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     loss_val, rmse_val \u001b[38;5;241m=\u001b[39m loss_func(q\u001b[38;5;241m=\u001b[39mq, p\u001b[38;5;241m=\u001b[39mp, b_user\u001b[38;5;241m=\u001b[39mb_user, b_item\u001b[38;5;241m=\u001b[39mb_item, b_global\u001b[38;5;241m=\u001b[39mb_global)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 86\u001b[0m, in \u001b[0;36mbias_update_factors\u001b[0;34m(encoded_users, encoded_items, ratings, q, p, b_user, b_item, b_global, lambda1, lambda2, lambda3, lambda4, lr)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):  \u001b[38;5;66;03m# For factor in k\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     q[encoded_user, f] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     84\u001b[0m         diff_deriv \u001b[38;5;241m*\u001b[39m p[encoded_item, f] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m lambda1 \u001b[38;5;241m*\u001b[39m q[encoded_user, f]\n\u001b[1;32m     85\u001b[0m     )\n\u001b[0;32m---> 86\u001b[0m     p[encoded_item, f] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     87\u001b[0m         diff_deriv \u001b[38;5;241m*\u001b[39m q[encoded_user, f] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m lambda2 \u001b[38;5;241m*\u001b[39m p[encoded_item, f]\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     90\u001b[0m b_user[encoded_user] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m*\u001b[39m(diff_deriv \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mlambda3 \u001b[38;5;241m*\u001b[39m b_user[encoded_user])\n\u001b[1;32m     91\u001b[0m b_item[encoded_item] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m*\u001b[39m(diff_deriv \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mlambda4 \u001b[38;5;241m*\u001b[39m b_item[encoded_item])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 8\n",
    "lambda1=lambda2=lambda3=lambda4=0.3\n",
    "\n",
    "unique_users = max(user_encoding.values()) + 1\n",
    "unique_items = max(item_encoding.values()) + 1\n",
    "\n",
    "np.random.seed(121017)  # NOQA: NPY002\n",
    "q = np.random.normal(size=unique_users*k).reshape(unique_users, k)  # NOQA: NPY002\n",
    "p = np.random.normal(size=unique_items*k).reshape(unique_items, k)  # NOQA: NPY002\n",
    "b_user = np.random.normal(size=unique_users)  # NOQA: NPY002\n",
    "b_item = np.random.normal(size=unique_items)  # NOQA: NPY002\n",
    "b_global = global_bias(train)\n",
    "\n",
    "q, p = bias_SGD(\n",
    "    encoded_users=train_users,\n",
    "    encoded_items=train_items,\n",
    "    ratings=train_ratings,\n",
    "    lambda1=lambda1,\n",
    "    lambda2=lambda2,\n",
    "    lambda3=lambda3,\n",
    "    lambda4=lambda4,\n",
    "    b_user=b_user,\n",
    "    b_item=b_item,\n",
    "    b_global=b_global,\n",
    "    q=q,\n",
    "    p=p,\n",
    "    epochs=10,\n",
    "    lr=0.01,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
